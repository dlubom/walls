<?xml version="1.0" encoding="UTF-8"?>
<topic template="Default" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../helpproject.xsd">
  <title translate="true">Choice of Mathematical Model</title>
  <keywords>
    <keyword translate="true">mathematical model</keyword>
  </keywords>
  <body>
    <header>
      <para styleclass="Heading1"><text styleclass="Heading1" translate="true">Choice of Mathematical Model</text></para>
    </header>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">When Walls does a least-squares adjustment, it assumes by default that the error </text><link displaytype="text" defaultstyle="true" type="topiclink" href="variance" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">variances</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> of the observed XYZ components of a traverse are each proportional to the traverse&apos;s total length. (For data format details see </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Variance_Assignments" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">Variance Assignments</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">.) No doubt we can come up with a more interesting approximation of variance -- maybe one that&apos;s more realistic. If we wanted to extract as much information as possible from our survey data, we could try constructing a detailed statistical model, where instrument standard deviations and other effects, like target positioning error, are specified as parameters. It so happens that the adjustment routine used by Walls was originally designed to support such a model, wherein a traverse misclosure, for example, can be dealt with mathematically as if it were more an effect of random compass errors than of random taping errors. (For details, see the article cited in </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Acknowledgements" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">History and Acknowledgements</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">, where matrix equivalents of the </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Statistical_Formulas" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">Statistical Formulas</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> used by Walls are described.)</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">Unfortunately, there are several drawbacks to this approach. Since the detailed model requires that a vector&apos;s error components be treated as </text><link displaytype="text" defaultstyle="true" type="topiclink" href="correlated" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">correlated,</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> the numerical operation involves the manipulation of 3x3 </text><link displaytype="text" defaultstyle="true" type="topiclink" href="covariance_matrix" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">covariance matrices</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> in place of scalars. The math is more memory consumptive and significantly slower; a </text><link displaytype="text" defaultstyle="true" type="topiclink" href="float" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">float</link><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">&#32;</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">operation in the Walls review dialog, for example, might take ten seconds instead of one. (This assumes that the same numerical algorithm is used. The direct sparse matrix method used by Walls is the fastest way I know of obtaining both the least-squares estimates and the additional statistics we need for data screening.)</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">A more serious drawback is the increased complexity of the program and its documentation. Would users appreciate a dialog featuring statistical parameters when it&apos;s unclear what the settings should be? The reliability of Suunto compass measurements surely drops off significantly with &quot;steep&quot; shots, but by how much? How large is the effect of target positioning error? Should our data format support a variance override consisting of </text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">six</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> parameters? (This would allow a vector to substitute for a traverse or subnetwork, as it can now, without affecting adjustment results.) &#160;And finally, should we continue to think of and treat separately (during float operations, for example) the horizontal and vertical dimensions of traverses? The correlated model by itself doesn&apos;t justify this separation, but the practice of ranking traverses by horizontal and vertical consistency has been so effective in finding blunders that Walls users have come to depend on it. The fact remains that some common types of blunders (bad azimuths and reversed signs on inclinations) destroy just one of the two kinds of consistency while leaving the other unaffected.</text></para>
    <para style="tabstops:24px left ;"></para>
    <para styleclass="Normal"><text styleclass="Normal" translate="true">Assuming cave surveyors are interested in these issues, and their surveys are good enough to be modeled this way, it&apos;s doubtful that introducing such complexity in Walls would be worth the effort. The end result would be data screening statistics much harder to get a feel for due to their dependence on subjective, or even arbitrary, parameter choices. Although </text><link displaytype="text" defaultstyle="true" type="topiclink" href="UVE" styleclass="Normal" translate="true">UVEs</link><text styleclass="Normal" translate="true"> could be obtained, they would be useless for objective comparisons between data sets unless we could all agree on a standard set of parameters and their values. Contrast this with inverse length-proportional weighting. While the latter amounts to a rather crude approximation of variance, it is supremely simple because it has no parameters -- that is, none that try to describe the errors in specific instrument readings. Instead of being sensitive to such assumptions, the UVEs can be regarded as variance scaling factors estimated from the data. Interpreted this way they can serve as objective measures of consistency. (See </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Variance_Assignments" anchor="RelativeVsTrue" styleclass="Normal" translate="true">Relative Variance vs True Variance</link><text styleclass="Normal" translate="true"> under Variance Assignments.)</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">To reduce the cost of least-squares computation, some cave mapping programs have assumed a watered down version of the detailed (correlated) variance model. This approach makes use of estimated measurement standard deviations to compute just the diagonal elements of a vector&apos;s covariance matrix, effectively assigning zero to the correlation terms. What makes this unattractive, in my view, is that for computational expediency alone we are giving up much of what&apos;s theoretically nice about the complete model. For one thing, assigning different variances to the two horizontal components while ignoring correlations causes adjustment results (e.g., the distances between points) to depend on which coordinate frame of reference we&apos;ve chosen. Also, parameter choices are not made any simpler. If we&apos;re going to go this far then why compromise on the math?</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">The more realistic our mathematical model, the more sensitive our statistics will be to </text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">departures</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> from the model -- that is, to blunders. But how many additional blunders would stand out if we used a different formula for assigning variances to vectors? A little experimentation with Walls should convince you there wouldn&apos;t be many. In the Suunto and tape </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Lot_Survey" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">tree survey</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> example, if you select any one of the 226 single-vector traverses and perturb an angle measurement by four degrees, the program will rank the traverse as the worst of the entire survey. It will also highlight the changed measurement and suggest a correction of about four degrees. In most cases, even a 2-degree perturbation would be noticeable. (Alternatively, you can examine the effect of changing a distance by a couple of feet.) &#160;It&apos;s true that in this example the tight control provided by numerous loops and short traverses is responsible for this sensitivity. Similar results would have been achieved with almost any variance function (e.g., weighting all vectors equally). When so few traverses have more than one vector, location estimates won&apos;t vary much either -- perhaps a few inches one way or another. </text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">Yet, when traverses are longer, it makes even less </text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">theoretical</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> difference which formula we use to weight individual vectors. Unless our model is so sophisticated that it treats instrument </text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">calibration error</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> as a random variable, the variance of a traverse is simply the sum of the variances, or covariance matrices, of the component vectors. (The assumption here is that the &quot;random&quot; measurement errors in different survey shots are reasonably small and independent of each other.) &#160;Consequently, as vector counts increase, the relative variances of traverses tend to resemble their relative lengths provided the distribution of shot lengths doesn&apos;t vary too much between traverses. Also, as directions change in a traverse, any asymmetry in the directional components of variance tends to be washed out.</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">It is often the case, unfortunately, that the longest traverses end up having the worst statistics, or </text><link displaytype="text" defaultstyle="true" type="topiclink" href="F_statistic" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">F-ratios</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">. While this might suggest to some that our model needs refinement, there&apos;s a much simpler explanation: Such traverses really </text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">are</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> bad. There are at least two good reasons why we can expect this. First, data screening is less effective in isolating blunders to specific vectors in long traverses; serious mistakes will go unnoticed. Second, surveys of long routes are often carried out by single teams and are more likely to be affected by systematic error. (See </text><link displaytype="text" defaultstyle="true" type="topiclink" href="LongTraverses" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">Error Propagation in Long Traverses</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">.) When our goal is data screening, we want to use a statistical model to expose such problems by way of contrast, not to simulate them. At some later stage of data processing, when we want up-to-date location estimates, then it&apos;s reasonable to assign a very large variance to a traverse we consider suspect. In Walls this is easily done with a </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Variance_Assignments" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">variance override</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">. &#160;Short of throwing the traverse out completely (or floating it), I know of no good way to handle such an assignment automatically. In fact, most of the large Walls databases I&apos;m aware of have several traverses that have been permanently </text><link displaytype="text" defaultstyle="true" type="topiclink" href="float" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">floated</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">.</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">The correlated component model still has some advantages worth pointing out. Perhaps its most important advantage is flexibility.</text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">&#32;</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">Transit surveys (i.e., turned angles) could be integrated with compass and tape data while giving proper statistical weight to the angle measurements. It would be possible to handle triangulations and </text><link displaytype="text" defaultstyle="true" type="topiclink" href="Bustamante" style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">missing measurements</link><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> in a more satisfying way. The breakdown of the UVE into components besides horizontal and vertical might provide interesting information about specific kinds of measurements. As already mentioned , instrument calibration error could be treated as a random variable and taken into account when deriving covariance matrices for traverses. In fact, instrument corrections could in some cases be treated as unknowns and estimated. (This can now be done iteratively with successive compilations.) Because much of the code has already been written, a future version of Walls might offer such a model as an option.</text></para>
    <para style="tabstops:24px left ;"></para>
    <para style="tabstops:24px left ;"><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true">However, it&apos;s unlikely that a cave survey would benefit in any practical sense from such a detailed model for an individual vector&apos;s variance, where we&apos;re presumably squeezing a little more information from the data set. As suggested above, the improvement in results owing to the </text><text style="font-family:Arial; font-size:9pt; font-style:italic; color:#000000;" translate="true">mere presence</text><text style="font-family:Arial; font-size:9pt; color:#000000;" translate="true"> of loops, or redundancy, in our cave surveys is likely to far outweigh in significance any benefit that could be derived from revising the statistical model. Implementing alternative models in Walls, as attractive as they might be theoretically, would surely break the most important rule of software design: Keep it as simple as practical considerations allow.</text></para>
    <para style="tabstops:24px left ;"></para>
  </body>
</topic>
