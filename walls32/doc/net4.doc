
            SOME NOTES ON THE STATISTICS PRODUCED BY WALLS

   Introduction
   ------------
   Among cave mapping programs WALLS is unusual in that certain special
   numbers (UveH, F/UveH, etc.) are presented in its output, either in
   tables or as lines of specific colors. From this we are supposed to
   conclude something about the survey data, such as whether or not
   blunders are present, and if so, whether specific locations in the
   data can be singled out. Unfortunately, the terminology is likely to
   be unfamiliar to most cave surveyors.

   As I hope to explain below, the special numbers are not some oddball
   invention of mine, but are standard formulas in applied statistics.
   They are used in many science and engineering disciplines, if not
   specifically cave surveying. The only novel thing in WALLS (at least
   as far as I know) is the numerical algorithm that does the actual
   computation and makes it possible to supply the numbers for very
   large data sets. This affects speed and use of computer resources,
   but has nothing to do with the theory.

   The following summary is presented for anyone curious about the
   specifics of a WALLS "compilation" of raw survey data. You may want
   to skip over most of the technical details unless you are familiar
   with statistics and matrices and are not too bothered by a
   superficial treatment and rather loose (but I hope not too confusing)
   terminology. Almost any basic statistics and/or linear algebra text
   should serve as a good background reference.

   Design Goals
   ------------
   I designed WALLS so that cave surveyors might find it a useful tool
   without necessarily knowing the theory behind it. I've long ago lost
   interest in proseletizing about statistics and least-squares. (In
   case you're interested to the point of actually implementing the
   algorithms, you can contact me by email, or check out Ref 1.) My
   feeling now is that claiming theoretical elegance is about as helpful
   as "You should buy our brand of binoculars because, we promise, the
   principles underlying its construction are very well founded." In
   other words, given its intended use, the program should be fairly
   easy to test.

   Here are the three basic cave surveying functions I had in mind when
   I started this project:

        1) Assist in data screening by exposing blunders, or by
           identifying parts of the project that should be resurveyed.
           To test this we should be able to introduce fake data errors
           and view their effects on the computed statistics.

        2) Average the non-discarded data in a reasonable way (assuming
           there is redundancy). Transforming conflicting data into a
           geometrically consistent map (i.e., where all loops close
           perfectly) is known as survey adjustment, or loop closing.
           This operation should provide measures of a survey's accuracy
           and/or quality.

        3) Assist in the planning, visualization, and/or drawing of
           publishable maps, especially those with digitized wall
           outlines that automatically conform to adjusted survey
           vectors. (Perhaps naming the program after this promised but
           unfinished feature will help bring it about.)

   Although these requirements seem important to me, most developers of
   cave mapping software have had different priorities. For example,
   while "average" is a fair characterization of an adjustment done by
   WALLS, "loop closing" describes all too literally a very different,
   but apparently still popular, technique for forcing cave survey data
   to be consistent. The least-squares approach has sometimes been
   criticized for blindly corrupting good data with bad, and has rarely,
   it seems, been appreciated for what I consider to be its greatest
   strength: providing the statistics useful for data screening.

   Finally, I'm not aware that the last feature (item 3) exists in
   another package, even though a description of my first implementation
   of wall drawing (on a minicomputer) was published in 1983 (ref 2).
   Surely there are others who see the need to make graphical output
   conform to survey vectors whose adjustments change as a large project
   evolves. Unfortunately, this requirement seems to rule out the
   exclusive use of commercial CAD software for maintaining cave maps
   digitally.

   To summarize, WALLS provides a few functions that are missing in the
   existing cave surveying software I've come across, but which I think
   are important. I don't mean to imply that these other programs are
   inferior, since several are in fact more capable overall. Developers,
   quite naturally, have chosen to emphasize different features. I'm
   afraid the program we would all like to have -- something of the same
   order of complexity as Excel or Corel Draw -- will remain impractical
   until the quality of programming tools improves drastically.

   Strings and Loop Systems
   ------------------------
   We need some terminogy for describing physically the objects we are
   trying to measure. In some archived discussions on the Internet I
   recently came across, one person complained about the tendency of
   caver/programmers to "strike out on their own" while ignoring the
   language and methods established by surveyors and published in
   textbooks. Considered offensive, for example, was the introduction of
   the term "string", apparently just as a replacement for "traverse."
   (If you are that person, please take a deep breath.)

   In the printed output of ELLIPSE (a mainframe-based mapping program
   no longer in use), strings were the smallest components of a survey
   network to which blunders could be localized due to inconsistency.
   Although the term appeared in a 1974 article on cave survey
   adjustment and has been familiar to users of ELLIPSE (and more
   recently, CAVEVIEW) for at least 15 years, it seems to have not caught
   on outside of Texas. Nor have I been able to dig up an equivalent
   term in the surveying or mathematical literature. But whatever we
   call them, these objects are as important conceptually as loops. The
   word "loop", by the way, is a very convenient substitute for "closed
   non-intersecting traverse", with traverse being any sequence of
   survey vectors (shots, legs, etc.) connected
   end-to-end.

   To obtain a better definition of string, we first need a precise
   notion of loop system, the domain where averaging takes place. A loop
   system is a maximal set of vectors in a network such that for any
   chosen pair of vectors in the set there is a loop containing them
   both. Dead end traverses and traverses that are the only routes
   between two parts of the network are excluded. So here is my working
   definition of string: Two vectors in a loop system belong to the same
   string if every loop that contains either one of them contains them
   both. Hence, any loop system is decomposable into a *unique* set of
   string components (not loops). In WALLS, these are precisely the
   components for which data screening statistics are computed.

   More often than not, a string is indeed just a traverse connecting
   two junctions in a loop system. But this is not true in general. The
   following diagram, for example, illustrates a 5-string loop system
   with one of the strings fragmented into upper and lower portions.

                               <diagram>

   If some vector's measurement were grossly in error, we could possibly
   narrow its location to a string, but not to a specific portion of the
   fragmented string without making further assumptions about the kind
   of error that is likely to have occurred. It is these strings, or
   string fragments, that are labeled "Adjusted Traverses" in the WALLS
   Review window. As you will discover, fragments of the same string will
   have identical statistics and identical "best corrections".

   Many surveys contain "isolated loops", each being a 1-string loop
   system (a single closed traverse). Finally, I will point out that
   "loop" in the sense we cavers use it is a "cycle" in graph theory,
   whereas the graph theoretic loop is something different.
   Unfortunately, I've so far been unable to find a simple equivalent
   term for string. (I'm open to suggestions.)

   A Model for Survey Adjustment
   -----------------------------
   When a station-to-station set of raw survey data is read from a text
   file, it is interpreted and eventually presented to the WALLS
   adjustment module as two basic data elements: an observed vector
   displacement (the East-North-Up components) and an associated error
   variance. The error variance, which I'll define later, is actually
   six numbers (3 component variances and 3 covariances), but can be
   thought of here as simply a positive quantity. In a survey containing
   loops, the variances determine how conflicting observations will be
   averaged to produce a consistent result, where all loops close
   perfectly.

   The larger the variance the less weight a vector will be given in the
   average and the larger its relative adjustment will be. If assigned a
   zero variance, for example, the vector will remain fixed in an
   adjustment operation. If given an infinite variance, the vector will
   play no role in the average, but will "float" in conformance with the
   remaining adjusted data.

   The advantage of making the inverse weights correspond as closely as
   possible to error variance is apparent when the survey is simulated
   mathematically. The averages, or rather least-squares estimates
   (LSEs) that result from such modeling, are popular because they are
   both easy to compute and have attractive properties. For example, the
   LSEs have the smallest variance in the class of linear unbiased
   estimates. That is, they tend to deviate least from their means,
   which are presumed to be the true values of the objects being
   measured. This supports the common sense notion that computing a
   weighted average is much better than arbitrarily throwing out
   redundant, or conflicting data. The LSEs are also popular because
   efficient programs for computing and interpreting them (confidence
   intervals, etc.) are widely available. The related statistics are the
   lengua franca for presenting experimental results.

   Linearizing Survey Measurements
   -------------------------------
   Linear model theory provides us with a straightforward way to set up
   our surveying problem starting with the assumed variances (squared
   standard deviations) of measurement errors. We must add to this the
   connectivity constraints imposed by a known network geometry. We
   know, for example, whether or not a location at the end (or start) of
   one traverse is the same as a location at the end of another
   traverse.

   Before getting into specific formulas, a brief outline of our plan
   might be helpful. First, there is the requirement of "linearizing"
   the observations, which in our case are the uncorrelated raw
   measurements that determine a vector. Bearing and inclination
   measurements, for example, are nonlinear functions of the point
   coodinates being estimated. Fortunately, except for a few special
   cases, we will have no difficulty deriving approximate linear
   functions. This is because we can ordinarily make two assumptions:

   1) the set of station-to-station measurements (e.g., distance,
      bearing, inclination) are sufficient to determine an initial
      estimate of the corresponding XYZ displacement vector (itself a
      linear function of the coordinates), and

   2) the unavoidable measurement errors are small enough so that within
      their expected range their combined effect on the estimated vector
      is approximately that of a linear perturbation.

   So we begin with three uncorrelated, nonlinear measurements and end
   with a linear 3-dimensional "observed" vector whose error components
   are small but, as we shall see, generally correlated.

   Note that we do *not* want to build into the model itself the
   important effects of systematic errors and blunders. In fact it is
   the effect of the latter, namely the *departure* from this idealistic
   model in actual data, that we hope to make obvious. This is not to
   say that no kinds of systematic errors can be modeled effectively.
   The adjustment model for ELLIPSE, for example, incorporated a
   systematic error in vertical target positioning and provided an
   estimate of its mean. The value of this was apparent in just one
   large surveying project I know of (Actun Kaua), where conditions
   (people, cave, and equipment) remained nearly constant and the
   correspondence between actual clinometer to target direction and that
   of FROM-TO in recorded data was maintained. I don't believe
   complicating WALLS with this feature is worth it.

   There still remains some special cases to consider. Take, for
   example, the problem of a missing distance measurement (or one that
   is thrown out), in which case condition 1) is not satisfied. How do
   we incorporate the information in what may be a perfectly good
   bearing and inclination? Well, we can often do this when the
   displacement is sufficiently determined from the sketch or by other
   vector observations in the network. Unfortunately, I know of no good
   way to handle all such cases automatically in software. Also, in the
   general case of missing measurements, non-linearity will often rear
   its ugly head. LSEs, even if obtainable via an iterative process, may
   no longer be optimal or even unique. (Another example in cave
   surveying is determining a position on the ceiling of a large room by
   multiple triangulation measurements.)

   When the linearized model is applied to real survey data, the end
   result is not only a set of estimates for the true locations (or
   rather the vectors that connect them), but also numbers that suggest
   how good the estimates are and how well the survey fits the model,
   either in total or piece-by-piece. Basically, the whole process is
   just the natural generalization of the familiar concept of average.
   The only difference is that the things ultimately averaged are not
   just independent measurements of a single unknown quantity, but are
   linearized functions of multiple unknown quantities.

   Defining Variance
   -----------------
   To continue with our derivation we nead a clearer notion of what
   variance actually is. Suppose our particular measurement is a number
   (or vector) representing a real object. Then its true (as opposed to
   estimated) error variance as the "average squared deviation from the
   mean" of infinitely many hypothetical measurements of the same
   object, each subject to error but all obtained under similar
   circumstances. This is a little abstract, but unlike some concepts we
   depend on (e.g., reasonable doubt), it leads directly to some useful
   statistical formulas.

   Our approach to cave survey adjustment will be to consider
   3-dimensional vectors, the displacements between pairs of
   geographical locations, as the observed objects. Each independent
   "measurement" in our model, therefore, is really an aggregate of
   actual raw measurements (e.g., distance, azimuth, and inclination)
   transformed into an XYZ Cartesian vector. Note that transit surveys,
   for example, do not fit neatly with this approach since they involve
   angle measurements that relate three, not two, geographical points.
   As a special observational type, such angles can be correctly dealt
   with in a future version of WALLS, but considering their rarity in
   cave surveys they are hardly worth complicating this discussion.

   Given that our basic observation is a vector, its true error
   variance, by our definition, is technically a symmetric
   "variance-covariance" matrix, not just a simple number. (In linear
   algebra we "square" a 3-dimensional column vector by multiplying it
   times its transpose to produce a 3x3 matrix.) For reasons to be
   explained below, WALLS provides the option of a simplified "scalar"
   model, which is equivalent to assuming that the vector's three error
   components are uncorrelated.

   A Generalized Vector Observation
   --------------------------------


   A Suggested Simplification
   --------------------------
   If we could assume that misclosures in cave surveying are due
   primarily to small, random, non-systematic errors in instrument
   readings and station positioning, and that further, it is important
   in cave surveying to obtain absolutely the best possible location
   accuracy given the available data, then an adjustment based on the
   3x3 matrix model for error variance would be worthwhile. This model
   would account for the different effects on accuracy, for example, of
   angle and length measurement errors. However, given that both
   assumptions are usually false, I believe that a simpler model,
   about as effective for data screening but requiring significantly
   less computing resources, is the most reasonable approach.

   Since the topic is nevertheless interesting to me and a few others, I
   expect to provide the matrix model as an option in a future version
   of WALLS, if only for experimental purposes. Its practical benefit,
   if any, is likely to be outweighed by its comparative slowness and
   complexity of use. For example, can we expect the user, in specifying
   model parameters, to provide good estimates of standard deviations
   (square roots of error variances) of tape and instrument readings and
   to account, as well, for the independent target positioning errors? A
   lack of consistency in this regard would surely make comparing
   different surveys even more difficult.

   In the simplified model, WALLS weights survey vectors by assigning to
   each separate XYZ error component a default variance that is
   proportional to the vector's total length, namely the number <total
   vector length>/100. Since we also make the simplifying assumption
   that the three components are uncorrelated, we need only assign to
   each vector one number, not the six numbers defining a symmetric
   matrix. The default variance, of course, can be overridden for
   individual vectors. If judged a blunder, for example, a vector (or
   travserse) can be flagged as having infinite variance.

   The default error variances are consistent with "longer vectors have
   larger expected errors", but are not very realistic otherwise. Our
   goal, however, is to produce UVE's (see below) of reasonable
   magnitude (less than one for good surveys and much greater than one
   for bad surveys) and to give approximately correct relative weighting
   to the strings, which are often long sequences of survey vectors.
   Whatever the formula we use for vectors, the variance of a string
   is correctly obtained by simply adding up the variance-covariance
   matrices of its constituent vectors. (This assumes that the string
   error contributed by one vector is independent of the errors
   contributed by other vectors.)

   It is therefore natural to regard variance as something proportional
   to length -- especially when we are dealing with long traverses. It
   is also quite conventional, since this formula happens to produce the
   traditionally popular "compass rule loop closure". Unlike the string
   and system statistics our method provides as a by-product (which are
   certainly not standard features of software in this category), the
   resulting location estimates, at least, should be identical to those
   of any program claiming to use the compass rule.

   Another advantage of the simple length model for variance is that it
   is less computationally demanding (requiring about 1/6 as much
   memory) while still being capable of exposing blunders and comparing
   different surveys objectively. The analogous statistics have proven
   to be quite useful in practice.

   Finally, with respect to accuracy of the final location estimates, it
   could be argued that any differences arising from a choice of error
   models would have little practical significance in low precision cave
   surveys, and would be swamped out anyway by non-obvious blunders,
   systematic (biased) errors, and various other factors. In this
   context, a particular error model affects accuracy only indirectly to
   the extent that data screening is potentially more effective.
   However, this is something for you (certainly not me) to decide. Your
   surveys may be of exceptional high quality.

   In fact, as computer memory and processing power have become
   dramatically less costly, there now seems to be little reason for a
   programmer not to provide the more general variance model as an
   optional method of adjustment. Could its increased sensitivity to
   blunders in better quality surveys be worthwhile? To be honest, I
   have not yet made up my mind. My hope is that WALLS can be used to
   help decide the issue.

   Practical Benefits
   ------------------
   Whenever we compute an average, there is at least an implied model
   for error variance. We can reasonably expect that the more realistic
   this model, the better use is made of the data at hand. While some
   simplification is inevitable, if the model is too crude we are simply
   throwing away useful information. Conversely, if the model is precise
   and based on accurate estimates of error variance, it has the
   potential for producing better (if not significantly better)
   estimates. Furthermore, the model *may* be significantly more
   sensitive to gross data errors, or blunders. This sensitivity is
   exactly what we want, since the statistics produced for individual
   strings, for example, can serve as a very effective tool for data
   screening. For cave surveying, I believe this is by far the most
   important benefit of using a realistic model.

   Estimating Survey Accuracy
   --------------------------
   Apart from their use in data screening and relative weighting, the
   assumed error variances have other implications. The result of an
   averaging process is itself an observation with a mathematically
   derivable error variance, making it feasible to combine the result
   with still more observations, and so forth. We can even derive
   so-called confidence intervals for estimated relative locations in a
   survey, although their practical meaning in our context is certainly
   debatable. For example: "If our model for measurement errors is
   realistic for surveys of this kind, of which this particular survey
   is a representative sample (i.e., without blunders), then the
   probability is .95 that the actual location of point A relative to
   point B is within X meters of the computed location."

   Obviously we can conclude very little from such numbers if the survey
   contains few if any loops and we have to rely on a priori estimates
   of error variance (and hope there are no blunders). However, in a big
   project involving hundreds of surveyed loops, post priori confidence
   intervals can be persuasive since they reflect the behavior already
   exhibited by a massive sample of data. In effect we are looking at
   the survey's internal consistency, and using this information to
   predict the result of just another closing traverse.

   Consider, for example, this sort of prediction: "If we were to survey
   a new independent traverse from point A to point B, with error
   variance V (possibly zero), then we would be adding another loop, or
   closure, to the project's data set. Based on the project's current
   degree of consistency (i.e., the quality of closures already
   obtained), there is a 95% chance that the difference between the new
   route to B and the existing network's (averaged) route to B will be
   less than X meters." Although calculating X by hand might be
   unthinkable, it's not hard to see the plausiblity and usefulness of
   such a prediction. I see no reason why a computer program can't
   provide it.

   Verifying the Model
   -------------------
   A statistical model does not have to be accepted on blind faith
   alone. A model for error variance is testable in the sense that when
   it is compared with a large sample of the data it presumably
   describes, certain numbers (statistics) based on the sample's
   internal consistency can be computed and compared with what would be
   expected if model were accurate and there were no blunders. A good
   way to gain confidence in the model is to introduce "mistakes" and
   check if their effects on the statistics would be noticeable enough
   for them to be singled out.

   Some Practical Recommendations
   ------------------------------
   Processing by WALLS improves accuracy primarily to the extent that
   bad measurements and data recording errors are often made conspicuous
   by the statistics produced for strings and loop systems. Once these
   mistakes, or "outliers", are removed, a cave survey should still
   benefit from the measurement redundancy inherent in a system of
   loops. Even if the weighting scheme is not exactly optimal, the
   accuracy of a least-squares adjustment, like that of most averages,
   should improve with the degree of redundancy in the data sample.

   In practice, the handling of data for a large project like Sistema
   Purificacion involves numerous runs of the error analysis portion of
   WALLS. Bad strings, made obvious by the statistics, are either
   corrected in the original data, resurveyed, or effectively discarded
   by giving them infinite variances (zero weights). At any particular
   time, when the data screening phase is complete, the pattern of
   assigned variances should more or less reflect our confidence in the
   remaining data. It is then reasonable to accept the resulting
   average, or adjustment, as our current best estimate of the cave. Any
   conclusions about accuracy resulting from this process (confidence
   regions or whatever) must earn our trust over time, either by
   independent verification or by repeated application of the program to
   similar data.

   How Multiple Program Runs can be Combined
   -----------------------------------------
   As has been suggested above, an "observed vector" need not be a
   direct measurement. Instead, it can represent an entire subnetwork of
   vectors joined to the rest of the system at two attachment points.
   The corresponding displacement and error variance for the subnetwork
   alone (ignoring correlations between error components) could have
   been obtained in a prior program run since WALLS will output not just
   displacement estimates (adjusted vectors), but also new error
   variances for the estimates themselves. For example, to obtain a
   displacement error variance between two arbitrary points in a
   network, we simply include in the original data a connecting dummy
   vector with infinite variance.

   UVE
   ---
   The Unit Variance Estimate (UVE) produced by our adjustment method is
   simply an estimated variance scaling factor based on the consistency
   of the data. Presumably, if a survey contains many loops and is free
   of blunders, a better approximation for a vector's true error
   variance is UVE*(vector length)/100, rather than the initial default.
   If we were to multiply all of the original variances by the UVE and
   rerun the program, a new UVE with value one should result. (The
   vector estimates, however, would not change.) The UVE can also be
   regarded as the mean-square error that is minimized in a
   least-squares type of average. As a measure of consistency, UVEs are
   useful for comparing the quality of different surveys or of different
   loop systems within a survey.

   Due to the nature of typical cave surveys, WALLS is set up to provide
   two separate UVE's for each loop system: one measuring the horizontal
   (XY) component consistency, UVE-H, and one measuring the vertical (Z)
   component consistency, UVE-V. Errors in compass readings, for
   example, will affect UVE-H, but not UVE-V. Vertical angle errors and
   distance errors can contribute to both statistics, but will often
   have a disproportionate affect on one or the other.

